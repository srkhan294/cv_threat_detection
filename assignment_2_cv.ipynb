{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation:-\n",
    "\n",
    "Tracking is process of identifying the positions of objects throughout multiple sequence of photos (i.e., video), tracking is getting the initial set of detections, assigning unique ids, and tracking them throughout frames of the video feed while maintaining the assigned ids. It's a 2 step process:-\n",
    "\n",
    "1. Detection and localization of the object in the frame using some object detector like YOLOv8, CenterNet, etc.\n",
    "2. Predicting the future motion of the object using its past information using a tracking algorithm.\n",
    "\n",
    "The deep_sort folder in the repo has the original deep sort implementation, complete with the Kalman filter, hungarian algorithm and feature extractor. But the original repo is built only for validating the algorithm with the MARS test dataset. So, we have written a custom class Tracker.py for ourself utilizing the original repo.\n",
    "\n",
    "Made Changes in generate_detections.py mudule to support latest version of tendsorflow.\n",
    "1. \"np.int\" is changed to \"int\"\n",
    "\n",
    "2.      self.session = tf.Session()\n",
    "        with tf.gfile.GFile(checkpoint_filename, \"rb\") as file_handle:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(file_handle.read())\n",
    "        tf.import_graph_def(graph_def, name=\"net\")\n",
    "        self.input_var = tf.get_default_graph().get_tensor_by_name(\n",
    "            \"net/%s:0\" % input_name)\n",
    "        self.output_var = tf.get_default_graph().get_tensor_by_name(\n",
    "            \"net/%s:0\" % output_name)\n",
    "\n",
    "    changed to:\n",
    "\n",
    "        self.session = tf.compat.v1.Session()\n",
    "        with tf.compat.v1.gfile.GFile(checkpoint_filename, \"rb\") as file_handle:\n",
    "            graph_def = tf.compat.v1.GraphDef()\n",
    "            graph_def.ParseFromString(file_handle.read())\n",
    "        tf.import_graph_def(graph_def, name=\"net\")\n",
    "        self.input_var = tf.compat.v1.get_default_graph().get_tensor_by_name(\n",
    "            \"%s:0\" % input_name)\n",
    "        self.output_var = tf.compat.v1.get_default_graph().get_tensor_by_name(\n",
    "            \"%s:0\" % output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Develop a computer vision solution to monitor and track people at an airport for security and operational efficiency.\n",
    "\n",
    "The solution uses yolov8n-pose for object detection and extraction of bone joint coordinates, it also uses deepSORT for tracking objects throughout the frames.\n",
    "Once we have the object detected and tracked we try to figure out whether he's a threat or not by estimating his hand position. Assuming that if his hand is held high and extended \n",
    "then he might be holding an object and pointing it at some direction which maybe a gun or knife, etc.\n",
    "Based on this assumption the code tries to figure out an extend arm position using coordinates from wrists, elbows and shoulder joints.\n",
    "We can further implement a threshold of frames/time after which which may consider a person as threat instead of instantly considering him/her as threat \n",
    "as soon as the criteria is met, which is the case with this demo solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.tracker import Tracker as DeepSortTracker\n",
    "from tools import generate_detections_ as gdet\n",
    "from deep_sort import nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Tracker:\n",
    "    tracker = None\n",
    "    encoder = None\n",
    "    tracks = None\n",
    "\n",
    "    def __init__(self):\n",
    "        max_cosine_distance = 0.4\n",
    "        nn_budget = None\n",
    "\n",
    "        encoder_model_filename = 'mars-small128.pb'\n",
    "\n",
    "        # distance metric to be used in the Hungarian algorithm for the data association problem\n",
    "        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "\n",
    "        # creating a multi-target tracker object with distance metric\n",
    "        self.tracker = DeepSortTracker(metric)\n",
    "\n",
    "        # object for generating image patches using bounding box for feature extraction later in the code\n",
    "        self.encoder = gdet.create_box_encoder(encoder_model_filename, batch_size=1)\n",
    "\n",
    "    # takes care of creation, keeping track of all tracks.\n",
    "    def update(self, frame, detections):\n",
    "        if len(detections) == 0:\n",
    "            self.tracker.predict()\n",
    "            self.tracker.update([])  \n",
    "            self.update_tracks()\n",
    "            return\n",
    "\n",
    "        bboxes = np.asarray([d[:-1] for d in detections])\n",
    "        bboxes[:, 2:] = bboxes[:, 2:] - bboxes[:, 0:2]\n",
    "        scores = [d[-1] for d in detections]\n",
    "\n",
    "        features = self.encoder(frame, bboxes)\n",
    "\n",
    "        dets = []\n",
    "        for bbox_id, bbox in enumerate(bboxes):\n",
    "            dets.append(Detection(bbox, scores[bbox_id], features[bbox_id]))\n",
    "\n",
    "        self.tracker.predict()\n",
    "        self.tracker.update(dets)\n",
    "        self.update_tracks()\n",
    "\n",
    "    def update_tracks(self):\n",
    "        tracks = []\n",
    "        for track in self.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlbr()\n",
    "\n",
    "            id = track.track_id\n",
    "\n",
    "            tracks.append(Track(id, bbox))\n",
    "\n",
    "        self.tracks = tracks\n",
    "\n",
    "\n",
    "class Track:\n",
    "    track_id = None\n",
    "    bbox = None\n",
    "\n",
    "    def __init__(self, id, bbox):\n",
    "        self.track_id = id\n",
    "        self.bbox = bbox\n",
    "\n",
    "'''\n",
    "Function to create a local coordinate system -> elbow as origin and shoulder, wrist as poits wrt it.\n",
    "'''\n",
    "def relative_pos(o,p2,p3):\n",
    "    p2=np.array(o)-np.array(p2)\n",
    "    p3=np.array(o)-np.array(p3)\n",
    "    return (p2,p3)\n",
    "\n",
    "'''\n",
    "Function to calculate angle between two vectors-> angle between wrist and shoulder, considering elbow as origin\n",
    "'''\n",
    "def angle_between(v1, v2):\n",
    "  cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "  return np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n",
    "\n",
    "'''\n",
    "Function to estimate whether a person is a threat or not\n",
    "based on location of his joints\n",
    "'''\n",
    "def estimate_threat(kypts):\n",
    "    flag=False\n",
    "    kypts=[[x,y] if p>0.5 else [0,0] for x,y,p in kypts]\n",
    "    rs,ls=kypts[6],kypts[5] #shoulders c,y coord\n",
    "    rw,lw=kypts[10],kypts[9] #wrist x,y coord\n",
    "    rel,lel=kypts[8],kypts[7] #elbow x,y coord\n",
    "    \n",
    "    '''\n",
    "    Check if any of the wrist is raised above the elbow level and the angle \n",
    "    formed between the shoulder and hand of the respective hand is greater than 90\n",
    "    then we may consider it as a threating gesture.\n",
    "    '''\n",
    "    if (rel!=[0,0]) & (rs!=[0,0]) & (rw!=[0,0]):\n",
    "        if rw[1]<=rel[1]:\n",
    "            p1,q1=relative_pos(rel,rw,rs)\n",
    "            angle=angle_between(q1,p1)\n",
    "            if angle>90:\n",
    "                flag=True\n",
    "                return flag\n",
    "    if (lel!=[0,0]) & (ls!=[0,0]) & (lw!=[0,0]):\n",
    "        if lw[1]<=lel[1]:\n",
    "            p1,q1=relative_pos(lel,lw,ls)\n",
    "            angle=angle_between(q1,p1)\n",
    "            if angle>90:\n",
    "                flag=True\n",
    "                return flag\n",
    "    return flag\n",
    "\n",
    "\n",
    "# Load the YOLOv8-pose model for object detection & gathering coordinate data of joints\n",
    "pose_model=YOLO('yolov8n-pose.pt')\n",
    "\n",
    "# Create a tracker object of DeepSORT\n",
    "tracker=Tracker()\n",
    "\n",
    "# List of random colors (BGR)\n",
    "colors=[(random.randint(0,255),random.randint(0,255),random.randint(0,200)) for j in range(10)]\n",
    "\n",
    "# Load input file\n",
    "cap = cv2.VideoCapture(0) # if not using webcam then replace 0 with video path\n",
    "success, frame= cap.read()\n",
    "\n",
    "while cap.isOpened():\n",
    "     success, frame= cap.read()\n",
    "     if success:\n",
    "          results=pose_model(frame,conf=0.2,classes=[0],show=False)\n",
    "          detections=[]\n",
    "          font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "          try:\n",
    "             for result in results:\n",
    "               for r,pr in zip(result.boxes.data.tolist(),result.keypoints.data.tolist()):\n",
    "                    x1,y1,x2,y2,probs,cls=r\n",
    "                    x1=int(x1)\n",
    "                    y1=int(y1)\n",
    "                    x2=int(x2)\n",
    "                    y2=int(y2)\n",
    "                    cls=int(cls)\n",
    "                    detections.append([x1,y1,x2,y2,probs])\n",
    "                    threat=estimate_threat(pr)\n",
    "                    if threat:\n",
    "                        cv2.putText(frame,'THREAT',(int(x1) , int(y1)),font,1,(0,0,255),3,cv2.LINE_AA)\n",
    "                    else:\n",
    "                        cv2.putText(frame,'SAFE',(int(x1) , int(y1)),font,1,(0,255,0),3,cv2.LINE_AA)\n",
    "\n",
    "               tracker.update(frame,detections)\n",
    "               for track in tracker.tracks:\n",
    "                    x1,y1,x2,y2=track.bbox\n",
    "                    id=track.track_id\n",
    "                    cv2.rectangle(frame,(int(x1) , int(y1)),(int(x2) , int(y2)), \n",
    "                            (colors[id % len(colors)]),3)\n",
    "                         \n",
    "          except Exception as e:\n",
    "               cap.release()\n",
    "               cv2.destroyAllWindows()\n",
    "               raise e\n",
    "           \n",
    "          cv2.imshow(\"YOLOv8 Inference\", frame)\n",
    "\n",
    "          if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "     else:\n",
    "          print('Failed!!')\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
